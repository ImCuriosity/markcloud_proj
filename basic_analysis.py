import pandas as pd
import os
import re
import sys
from collections import defaultdict
from io import StringIO

# --- ì„¤ì • ---
DATA_DIR = './data/'
OUTPUT_DIR = './outputs/basic/'
LOG_FILE = os.path.join(OUTPUT_DIR, 'analysis_results.txt')

NICE_CLASS_DESC = {
    '1': 'í™”í•™í’ˆ', '2': 'ë„ë£Œ/ì—¼ë£Œ', '3': 'í™”ì¥í’ˆ/ì„¸ì •ì œ', '4': 'ì‚°ì—…ìš© ìœ ì§€', 
    '5': 'ì•½ì œ/ì˜ì•½í’ˆ/ìœ„ìƒì¬', '6': 'ê¸ˆì†ì œí’ˆ', '7': 'ê¸°ê³„/ê³µì‘ê¸°ê³„', '8': 'ìˆ˜ê³µêµ¬', 
    '9': 'ê³¼í•™/ì „ì/ì»´í“¨í„° í•˜ë“œì›¨ì–´ ë° ì†Œí”„íŠ¸ì›¨ì–´', '10': 'ì˜ë£Œìš© ê¸°ê¸°/ìš©í’ˆ', '11': 'ì¡°ëª…/ëƒ‰ë‚œë°©/ê±´ì¡° ì¥ì¹˜', 
    '12': 'íƒˆê²ƒ', '14': 'ê·€ê¸ˆì†/ë³´ì„/ì‹œê³„', '16': 'ì¢…ì´/ë¬¸êµ¬', '18': 'í”¼í˜/ê°€ì£½ì œí’ˆ', 
    '20': 'ê°€êµ¬/ê±°ìš¸/ì•¡ì', '21': 'ê°€ì •ìš©êµ¬/ìœ ë¦¬/ìê¸°', '25': 'ì˜ë¥˜/ì‹ ë°œ/ëª¨ì', 
    '29': 'ê°€ê³µì‹í’ˆ/ìœ¡ë¥˜/ìœ ì œí’ˆ', '30': 'ì»¤í”¼/ì°¨/ì œê³¼', '31': 'ë†ì‚°ë¬¼/ë¹„ê°€ê³µ ì‹í’ˆ/ë™ë¬¼ì‚¬ë£Œ', 
    '35': 'ê´‘ê³ /ê²½ì˜ê´€ë¦¬', '36': 'ë³´í—˜/ê¸ˆìœµ', '38': 'í†µì‹ ', '41': 'êµìœ¡/ì˜¤ë½/ìŠ¤í¬ì¸ ', 
    '42': 'ê³¼í•™/ê¸°ìˆ  ì„œë¹„ìŠ¤/IT ì„œë¹„ìŠ¤', '43': 'ìŒì‹ì ì—…/ì„ì‹œìˆ™ë°•ì—…', '44': 'ì˜ë£Œ/ë¯¸ìš©/ë†ì—… ì„œë¹„ìŠ¤',
    '45': 'ë²•ë¥ /ë³´ì•ˆ/ê°œì¸ ì„œë¹„ìŠ¤', 'ê¸°íƒ€': 'ê¸°íƒ€ ë¶„ë¥˜'
}


# pandas DataFrameì´ë‚˜ Seriesë¥¼ printí•  ë•Œ, í„°ë¯¸ë„/íŒŒì¼ ì¶œë ¥ í¬ë§·ì„ ì¡°ì •
# DataFrameì˜ ëª¨ë“  í–‰ê³¼ ì—´ì„ ì¶œë ¥í•˜ë„ë¡ ì„¤ì • (Truncation ë°©ì§€)
pd.set_option('display.max_rows', None)
pd.set_option('display.max_columns', None)
pd.set_option('display.width', 1000)
pd.set_option('display.colheader_justify', 'left')
pd.set_option('display.precision', 2) # ì†Œìˆ˜ì  ìë¦¬ìˆ˜ ì¡°ì •

# ê¸°ì¡´ í•¨ìˆ˜ë“¤ (load_all_data, preprocess_data, analyze_time_series, analyze_category, analyze_comparison, analyze_text)
# ëŠ” ìš”ì²­í•˜ì‹  ëŒ€ë¡œ ìœ ì§€ë©ë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” íŒŒì¼ í¬ê¸° ë¬¸ì œë¡œ ìƒëµí•©ë‹ˆë‹¤.

def load_all_data(data_dir):
    """data í´ë” ë‚´ì˜ ëª¨ë“  .xlsx íŒŒì¼ì„ ë¡œë“œí•˜ê³  í•˜ë‚˜ì˜ DataFrameìœ¼ë¡œ í†µí•©í•©ë‹ˆë‹¤."""
    all_dfs = []
    file_list = [f for f in os.listdir(data_dir) if f.endswith('.xlsx')]
    
    country_map = {f: f.split('_')[0].replace('DATA.xlsx', '').replace('.xlsx', '') for f in file_list}

    print("### 1. ë°ì´í„° ë¡œë“œ ë° í†µí•© ì‹œì‘ ###")
    for file_name in file_list:
        file_path = os.path.join(data_dir, file_name)
        country_name = country_map.get(file_name, 'Unknown')
        
        try:
            df = pd.read_excel(file_path)
            df['êµ­ê°€'] = country_name
            all_dfs.append(df)
            print(f"-> ë¡œë“œ ì™„ë£Œ: {file_name} (ì´ {len(df)} í–‰)")
        except Exception as e:
            print(f"-> ì˜¤ë¥˜ ë°œìƒ: {file_name} ë¡œë“œ ì‹¤íŒ¨ - {e}")

    if all_dfs:
        combined_df = pd.concat(all_dfs, ignore_index=True)
        print(f"\nì´ í†µí•© ë°ì´í„°í”„ë ˆì„ í¬ê¸°: {len(combined_df)} í–‰")
        return combined_df
    else:
        print("ê²½ê³ : ë¡œë“œí•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return pd.DataFrame()


def preprocess_data(df):
    """í†µí•© ë°ì´í„°ì— ëŒ€í•œ ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
    print("\n### 2. ë°ì´í„° ì „ì²˜ë¦¬ ###")
    
    df['ì¶œì›ì¼ì'] = pd.to_datetime(df['ì¶œì›ì¼ì'], errors='coerce')
    print(f"-> 'ì¶œì›ì¼ì' ì»¬ëŸ¼ì„ datetime í˜•ì‹ìœ¼ë¡œ ë³€í™˜ ì™„ë£Œ. (ë³€í™˜ ë¶ˆê°€í•œ ê°’: {df['ì¶œì›ì¼ì'].isna().sum()}ê°œ)")
    
    df['ì£¼ìš”_ë¥˜'] = df['ë¥˜'].astype(str).apply(lambda x: x.split('//')[0].strip())
    df['ì£¼ìš”_ë¥˜'] = df['ì£¼ìš”_ë¥˜'].str.extract(r'(\d+)').fillna('ê¸°íƒ€').astype(str)
    print("-> 'ë¥˜' ì»¬ëŸ¼ ì •ì œí•˜ì—¬ 'ì£¼ìš”_ë¥˜' ì»¬ëŸ¼ ìƒì„± ì™„ë£Œ.")
    
    df['ìƒí‘œëª…ì¹­'].fillna('(ìƒí‘œëª…ì¹­ ì •ë³´ ì—†ìŒ)', inplace=True)
    print("-> 'ìƒí‘œëª…ì¹­' ì»¬ëŸ¼ ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì™„ë£Œ.")
    
    return df


def analyze_time_series(df):
    """ì—°ë„ë³„ ì¶œì› íŠ¸ë Œë“œë¥¼ ë¶„ì„í•˜ê³ , êµ­ê°€ë³„ Top 5 ì¶œì› ì—°ë„ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤."""
    print("\n### 3. ì‹œê³„ì—´ íŠ¸ë Œë“œ ë¶„ì„ ###")
    
    df_ts = df.dropna(subset=['ì¶œì›ì¼ì']).copy()
    df_ts['ì¶œì›ì—°ë„'] = df_ts['ì¶œì›ì¼ì'].dt.year
    
    yearly_counts = df_ts.groupby(['ì¶œì›ì—°ë„', 'êµ­ê°€']).size().reset_index(name='ì¶œì›ìˆ˜')
    
    print("ğŸ’¡ êµ­ê°€ë³„ ì¶œì› ê±´ìˆ˜ Top 5 ì—°ë„:\n")
    
    top_5_yearly_counts = yearly_counts.groupby('êµ­ê°€').apply(
        lambda x: x.sort_values(by='ì¶œì›ìˆ˜', ascending=False).head(5)
    ).reset_index(drop=True)
    
    for country in top_5_yearly_counts['êµ­ê°€'].unique():
        print(f"**--- {country} ---**")
        output_df = top_5_yearly_counts[top_5_yearly_counts['êµ­ê°€'] == country].sort_values(
            by='ì¶œì›ìˆ˜', ascending=False
        ).reset_index(drop=True)
        print(output_df[['ì¶œì›ì—°ë„', 'ì¶œì›ìˆ˜']])
        print("---")

    max_year = yearly_counts['ì¶œì›ì—°ë„'].max()
    start_year = max_year - 4 
    
    cagr_results = []
    for country in yearly_counts['êµ­ê°€'].unique():
        country_data = yearly_counts[yearly_counts['êµ­ê°€'] == country]
        
        start_count_row = country_data[country_data['ì¶œì›ì—°ë„'] == start_year]
        end_count_row = country_data[country_data['ì¶œì›ì—°ë„'] == max_year]
        
        if not start_count_row.empty and not end_count_row.empty:
            beginning_value = start_count_row['ì¶œì›ìˆ˜'].iloc[0]
            ending_value = end_count_row['ì¶œì›ìˆ˜'].iloc[0]
            n = max_year - start_year
            
            if beginning_value > 0:
                cagr = (ending_value / beginning_value) ** (1/n) - 1
                cagr_results.append({'êµ­ê°€': country, f'{start_year}-{max_year} CAGR': f'{cagr * 100:.2f}%'})

    print(f"\nğŸ’¡ ìµœê·¼ 5ë…„ CAGR ({start_year}ë…„ ëŒ€ë¹„ {max_year}ë…„):\n", pd.DataFrame(cagr_results))


def analyze_category(df):
    """ì£¼ìš” ë¥˜(Class)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ êµ­ê°€ë³„ ì‚°ì—… íŠ¹ì„±ì„ ë¶„ì„í•˜ê³  ë¥˜ ì„¤ëª…ì„ í•¨ê»˜ í‘œì‹œí•©ë‹ˆë‹¤."""
    print("\n### 4. ì‚°ì—… ë° ë¶„ë¥˜ ë¶„ì„ (ì£¼ìš”_ë¥˜ ê¸°ì¤€) ###")
    
    country_class_counts = df.groupby('êµ­ê°€')['ì£¼ìš”_ë¥˜'].value_counts(normalize=True).mul(100).rename('ë¹„ì¤‘(%)').reset_index()
    
    country_class_counts['ë¥˜_ì„¤ëª…'] = country_class_counts['ì£¼ìš”_ë¥˜'].astype(str).map(NICE_CLASS_DESC).fillna('ì„¤ëª… ì—†ìŒ')
    
    top_classes = country_class_counts.groupby('êµ­ê°€').head(5).sort_values(by=['êµ­ê°€', 'ë¹„ì¤‘(%)'], ascending=[True, False])

    print("ğŸ’¡ êµ­ê°€ë³„ ìƒìœ„ 5ê°œ ì£¼ìš”_ë¥˜ ë¹„ì¤‘ ë° ì„¤ëª…:\n")
    
    for country in top_classes['êµ­ê°€'].unique():
        print(f"**--- {country} ---**")
        output_df = top_classes[top_classes['êµ­ê°€'] == country][['ì£¼ìš”_ë¥˜', 'ë¥˜_ì„¤ëª…', 'ë¹„ì¤‘(%)']]
        print(output_df)
        print("---")


def analyze_comparison(df):
    """êµ­ê°€ë³„ í¬íŠ¸í´ë¦¬ì˜¤ ë‹¤ì–‘ì„±ê³¼ ì§€ì •ìƒí’ˆ ê°œìˆ˜ë¥¼ ë¹„êµ ë¶„ì„í•©ë‹ˆë‹¤."""
    print("\n### 5. ê¸€ë¡œë²Œ ë¹„êµ ë¶„ì„ ###")
    
    diversity_data = []
    for country in df['êµ­ê°€'].unique():
        country_df = df[df['êµ­ê°€'] == country]
        unique_classes = sorted(country_df['ì£¼ìš”_ë¥˜'].unique().tolist())
        
        class_with_desc = []
        for class_code in unique_classes:
            desc = NICE_CLASS_DESC.get(class_code, 'ì„¤ëª… ì—†ìŒ')
            class_with_desc.append(f"{class_code} ({desc})")
        
        diversity_data.append({
            'êµ­ê°€': country,
            'ê³ ìœ _ë¥˜_ê°œìˆ˜': len(unique_classes),
            'í¬í•¨ëœ_ë¥˜_ì¢…ë¥˜': ', '.join(class_with_desc)
        })
        
    diversity_df = pd.DataFrame(diversity_data).sort_values(by='ê³ ìœ _ë¥˜_ê°œìˆ˜', ascending=False).reset_index(drop=True)
    print("ğŸ’¡ êµ­ê°€ë³„ í¬íŠ¸í´ë¦¬ì˜¤ ë‹¤ì–‘ì„± (ê³ ìœ  ë¥˜ ê°œìˆ˜ ë° ì¢…ë¥˜):\n", diversity_df)
    
    df['ì§€ì •ìƒí’ˆ_ê°œìˆ˜'] = df['ì§€ì •ìƒí’ˆ'].astype(str).apply(lambda x: len(re.split(r'//|,|\n', x)))
    
    avg_goods = df.groupby('êµ­ê°€')['ì§€ì •ìƒí’ˆ_ê°œìˆ˜'].mean().sort_values(ascending=False).reset_index(name='í‰ê· _ì§€ì •ìƒí’ˆ_ìˆ˜')
    print("\nğŸ’¡ êµ­ê°€ë³„ ì¶œì› ê±´ë‹¹ í‰ê·  ì§€ì •ìƒí’ˆ ìˆ˜:\n", avg_goods)


def analyze_text(df):
    """ìƒí‘œëª… ê¸¸ì´, ìƒí‘œëª… í‚¤ì›Œë“œ, ì§€ì •ìƒí’ˆ í‚¤ì›Œë“œ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
    print("\n### 6. í…ìŠ¤íŠ¸ ë§ˆì´ë‹ (Text Mining & NLP) ###")
    
    df['ìƒí‘œëª…_ê¸¸ì´'] = df['ìƒí‘œëª…ì¹­'].astype(str).apply(lambda x: len(re.sub(r'\s|\(|\)', '', x)))
    length_summary = df.groupby('êµ­ê°€')['ìƒí‘œëª…_ê¸¸ì´'].agg(['mean', 'median', 'min', 'max']).sort_values(by='mean', ascending=False)
    print("ğŸ’¡ 1-1. êµ­ê°€ë³„ ìƒí‘œëª… ê¸¸ì´ ìš”ì•½ í†µê³„:\n", length_summary)
    
    print("\nğŸ’¡ 2. êµ­ê°€ë³„ ìƒí‘œëª… ìƒìœ„ í‚¤ì›Œë“œ íŠ¸ë Œë“œ (ë¹ˆë„ ë¶„ì„):")
    
    STOP_WORDS = ['the', 'and', 'of', 'for', 'in', 'a', 'trade', 'mark', 'ltd', 'inc', 'co', 'group']
    
    for country in df['êµ­ê°€'].unique():
        country_names = df[df['êµ­ê°€'] == country]['ìƒí‘œëª…ì¹­'].astype(str).str.lower()
        
        all_words = []
        for name in country_names:
            words = re.sub(r'[^a-z0-9\s]', '', name).split()
            words = [word for word in words if word not in STOP_WORDS and len(word) > 2]
            all_words.extend(words)
            
        word_counts = pd.Series(all_words).value_counts().head(10)
        
        if not word_counts.empty:
            print(f"**--- {country} ìƒí‘œëª… Top 10 í‚¤ì›Œë“œ ---**")
            print(word_counts)
        else:
            print(f"**--- {country} ---** í‚¤ì›Œë“œ ë¶„ì„ ë¶ˆê°€ ë˜ëŠ” ë°ì´í„° ë¶€ì¡± (ì£¼ë¡œ ë¹„ì˜ë¬¸ ë°ì´í„°ì¸ ê²½ìš°)")
    
    print("\nğŸ’¡ 3. ì§€ì •ìƒí’ˆ ìƒìœ„ í‚¤ì›Œë“œ ë¶„ì„ (êµ­ê°€ë³„ Top 10):")
    
    for country in df['êµ­ê°€'].unique():
        country_goods = df[df['êµ­ê°€'] == country]['ì§€ì •ìƒí’ˆ'].astype(str).str.lower()
        
        keyword_counts = defaultdict(int)
        
        for text in country_goods:
            goods_list = re.split(r'[//,\n]', text)
            for good in goods_list:
                good = good.strip()
                if good and len(good) > 5 and good not in STOP_WORDS:
                    keyword_counts[good] += 1
                    
        top_goods_keywords = pd.Series(keyword_counts).sort_values(ascending=False).head(10)
        
        if not top_goods_keywords.empty:
            print(f"\n**--- {country} ì§€ì •ìƒí’ˆ Top 10 í‚¤ì›Œë“œ ---**")
            print(top_goods_keywords)
        else:
            print(f"\n**--- {country} ---** ì§€ì •ìƒí’ˆ í‚¤ì›Œë“œ ë¶„ì„ ë¶ˆê°€")


# --- ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (ì¶œë ¥ íŒŒì¼ ì €ì¥ ë¡œì§ ì¶”ê°€) ---
if __name__ == "__main__":
    
    # 1. ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    if not os.path.exists(OUTPUT_DIR):
        os.makedirs(OUTPUT_DIR)
    
    # í‘œì¤€ ì¶œë ¥(sys.stdout)ì„ ë©”ëª¨ë¦¬ ë²„í¼ë¡œ ë¦¬ë‹¤ì´ë ‰ì…˜
    original_stdout = sys.stdout
    string_buffer = StringIO()
    sys.stdout = string_buffer

    try:
        # ë¶„ì„ ì‹¤í–‰
        # 1. ëª¨ë“  ë°ì´í„° ë¡œë“œ ë° í†µí•©
        all_data = load_all_data(DATA_DIR)

        if not all_data.empty:
            # 2. ë°ì´í„° ì „ì²˜ë¦¬
            processed_data = preprocess_data(all_data)

            # 3. ì‹œê³„ì—´ íŠ¸ë Œë“œ ë¶„ì„
            analyze_time_series(processed_data)
            
            # 4. ì‚°ì—… ë° ë¶„ë¥˜ ë¶„ì„
            analyze_category(processed_data)

            # 5. ê¸€ë¡œë²Œ ë¹„êµ ë¶„ì„
            analyze_comparison(processed_data)

            # 6. í…ìŠ¤íŠ¸ ë§ˆì´ë‹
            analyze_text(processed_data)
            
            print("\n--- ê¸°ë³¸ ë¶„ì„ ì™„ë£Œ ---")

    finally:
        # 2. íŒŒì¼ ì €ì¥
        analysis_results = string_buffer.getvalue()
        with open(LOG_FILE, 'w', encoding='utf-8') as f:
            f.write(analysis_results)
        
        # 3. í‘œì¤€ ì¶œë ¥ ë³µì›
        sys.stdout = original_stdout
        
        print(f"\nâœ… ë¶„ì„ ê²°ê³¼ê°€ '{LOG_FILE}' íŒŒì¼ì— ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(f"   (ì¶œë ¥ ë””ë ‰í† ë¦¬: {os.path.abspath(OUTPUT_DIR)})")